\chapter{Machine Learning for Interaction Region Local Coupling} % Main chapter title
\label{chapter:ml_local_coupling} % For referencing the chapter elsewhere, use \cref{Chapter:ML_Local_Coupling}

Machine learning methods have found their application in a variety of fields ranging including technology development, scientific research, medical diagnosis and business insights.
The now widespread application of machine learning techniques demonstrates their applicability to various challenges and tasks.

In recent years machine learning has been successfully applied to various areas of accelerator physics~\cite{}, including at the LHC~\cite{PRAB:Fol:Detection_Faulty_BPMs,EPJP:Fol:Supervised_Learning_Reconstruction_Magnet_Errors,PHD:Fol:Application_ML_Beam_Optics}.
The primary purpose of the work presented hereafter is to explore the possibility of applying machine learning techniques to the subject of local IR coupling in the LHC.

\section{Relevant Concepts of Machine Learning}

Machine learning can be seen as the intersection of statistics and computer science.
T. Mitchell illustrates the key characteristics of machine learning as \textit{"machine learning techniques aim to build computer programs and algorithms that automatically improve with experience by learning from examples with respect to some class of task and performance measure, without being explicitly programmed"}~\cite{BOOK:Mitchell:Machine_Learning}.
A key characteristic distinguishing machine learning from conventional programming is the program's ability to automatically achieve performance improvements based on provided data.

This section provides a quick introduction to relevant paradigms and concepts of machine learning, primarily derived from~\cite{BOOK:Mitchell:Machine_Learning,BOOK:Hastie:Elements_Statistical_Learning} where one can find detailed discussions on the subject.
The focus hereafter is kept only on particular algorithms used in the applications presented in this chapter.

\subsection{Defining a Machine Learning Task}

In the context of machine learning, the \textit{learning} refers to the process of performance improvement.
Generally speaking, a given model can improve its performance by adjusting its parameters with respect to provided data in order to refine the approximation function that is derived from said data. 
It is important to note that a trained model will always remain an approximation of the relationship in the underlying data, but that with a good enough training this approximation should be sufficiently reliable for the specific task at hand.
The requirements on the model's performance in terms of accuracy, precision and reliability depend on a particular task and application domain.

The chosen performance criteria, the type of experience to be obtained via learning and the specific approach and algorithm are specific to the task that should be learned.
These three parameters - task, measure of performance and source of experience - are the fundamentals of a well-defined machine learning problem.
In practice, it is reflected in the selection of appropriate learning method and function approximation algorithm, definition of the loss function and its acceptable values, as well as the preparation of data.
The latter usually appears to be non-trivial and can require dedicated techniques and algorithms e.g. for feature extraction or correlation analysis.

\subsection{Supervised and Unsupervised Learning}

Two main approaches are available in machine learning depending on the problem definition and the availability and structure of learning example.

\intro{Unsupervised learning} is characterized by the use of unlabeled datasets.
Unsupervised learning algorithms solve tasks where only input data is available, and are used to analyze and cluster inputs by discovering patterns in data without the need for human intervention, hence them being \textit{unsupervised}.
They are suitable for tasks such as anomaly detection, signal denoising, dimensionality reduction, and feature extraction.
% They are used for tasks such as clustering, association and dimensionality reduction.
% Clustering groups unlabeled data based on their similarities or differences, association finds relationships between variables in a given dataset and dimensionality reduction reduces the number of data inputs to a manageable size while also preserving the data integrity.

\intro{Supervised learning} is characterized by the use of labeled datasets.
These datasets are used to train - or \textit{supervise} - the model to learn how to accurately classify data or predict outcomes.
During the training, predictions are computed from the inputs and compared to the known corresponding outputs.
By adjusting the parameters of the model - or approximation function - the quantified difference between computed predictions and correct outputs - the so-called \textit{loss function} - is minimized.
% By providing known labeled inputs and outputs, the model can measure its accuracy and adjust its parameters to improve during the learning.
Supervised learning is typically used for \intro{classification} and \intro{regression} tasks.
Classification tasks aim to accurately assign test data into specific categories, and regression tasks understand aim to understand and approximate the relationship between dependent and independent variables in order to predict the outcome of a given input.

\subsection{Generalization and Overfitting}

\subsection{Regression and Classification}

% \subsection{Neural Networks}

% \subsection{Decision Trees and Ensemble Methods}


\section{Identification of Local Coupling Sources}

In the previous chapter, the negative impact of local IR linear coupling in the LHC has been extensively discussed, as well as the necessity of its identification and mitigation.
The precise knowledge of a coupling source's location and relative strength would be a valuable asset for further correction, however it was illustrated in \cref{section:current_correction_methods_and_their_limitations} how simply looking at patterns and jumps in the coupling RDTs was not always sufficient to accurately pinpoint the  location of a source, nor can it be used in locations with little instrumentation or unfavorable conditions.
As machine learning techniques have found their application in a wide range of particle accelerator control tasks in the past~\cite{IPAC:Fol:Machine_Learning_Methods_for_Optics_Measurements_and_Corrections_at_LHC, IEEE:Edelen:Neural_Networks_Modeling_Control_Particle_Accelerators, EPAC:Bozoki:Neural_Networks_Orbit_Control_Accelerators, IPAC:Meier:Orbit_Correction_Studies_using_Neural_Networks, EPAC:Kijima:Beam_Diagnostic_System_for_Accelerator_using_Neural_Networks, PRAB:Fol:Detection_Faulty_BPMs, EPJP:Fol:Supervised_Learning_Reconstruction_Magnet_Errors}, a study was made to explore the possibility of applying machine learning techniques to the detection of local IR coupling sources in the LHC.

In order to perform a prediction of betatron coupling sources' locations, one first needs to compute the \(f_{1001}\) and \(f_{1010}\) coupling RDTs which will serve as input data for the model.
The strength and variations of the coupling RDTs throughout the machine is then used to estimate the location of coupling sources and their relative strengths.

In terms of machine learning, this task can be defined as a regression problem that can be solved by training a model using measurements and corresponding solutions.
Such a regression model requires a large data set in order to be able to generalize and produce reliable results.
As from the real machine the location of sources is unknown, no real-world data is available for the training of machine learning models.

\subsection{Dataset Generation}

In order to create a training dataset, simulations were performed with the MAD-X~\cite{CODE:MADX_guide} code, in which random rotations around the \(s\)-axis are introduced into individually powered IR quadrupoles in each IR.
The tilt generates a skew quadrupolar component at the affected element and thus turns it into a source of coupling.
It has to be noted though, that in simulations only quadrupole tilts - given by the \(\mathrm{DPSI}\) variable in MAD-X - were used to generate coupling sources, which means other potential sources such as feed-down from higher order magnets were ignored in these studies.

While simulating the data, the introduced tilt components are the training inputs in the dataset and the produced coupling RDTs generated from the perturbed optics functions are the outputs.
The data was generated for both Beam 1 and 2 using the \(\beta^{\ast}=\)~\qty{30}{\centi\meter} optics.
\Cref{figure:qqqqqb1,figure:qqqqqb2} shows the reconstructed coupling RDTs for a given simulation in which a truncated Gaussian distribution of tilts was assigned to all independently powered IR quadrupoles Q\num{1} to Q\num{11} in IPs \numlist{1;2;5;8}.
The standard deviation of the applied tilt errors Gaussian distribution was aligned with expected values from the element alignment precision in the LHC, after discussions with the alignment group.

\begin{figure}[!htb]
    \centering
    \includegraphics*[width=0.9\textwidth]{Figures/ML_for_IR_Coupling/ml_local_sources_b1_rdts.pdf}
    \caption{Beam~\num{1} coupling RDTs \(f_{1001}\) (\textcolor{mplblue}{blue}) and \(f_{1010}\) (\textcolor{mplorange}{orange}) after the implementation of tilt errors in the independently powered IR quadrupoles Q\num{1} to Q\num{11} in IRs \numlist{1;2;5;8}, for the \qty{6.8}{\tera\electronvolt} and \(\beta^{\ast}=\)~\qty{30}{\centi\meter} optics.}
    \label{figure:qqqqqb1}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics*[width=0.9\textwidth]{Figures/ML_for_IR_Coupling/ml_local_sources_b2_rdts.pdf}
    \caption{Beam~\num{2} coupling RDTs \(f_{1001}\) (\textcolor{mplblue}{blue}) and \(f_{1010}\) (\textcolor{mplorange}{orange}) after the implementation of tilt errors in the independently powered IR quadrupoles Q\num{1} to Q\num{11} in IRs \numlist{1;2;5;8}, for the \qty{6.8}{\tera\electronvolt} and \(\beta^{\ast}=\)~\qty{30}{\centi\meter} optics.}
    \label{figure:qqqqqb2}
\end{figure}

Each simulated sample of the dataset is obtained by applying the following steps in simulations:
\begin{enumerate}
    \item A truncated Gaussian distribution of tilt errors (\(\mathrm{DPSI}\)) is applied to quadrupoles Q\num{1} to Q\num{11} in IRs \numlist{1;2;5;8} for Beam~\num{1}.
    \item Quadrupoles located outside the IRs are excluded as these sources can be well corrected by other means.
    \item The coupling RDTs \(f_{1001}\) and \(f_{1010}\) are calculated at each BPM from Twiss functions for Beam~\num{1}.
    \item The \(\mathrm{DPSI}\) values for triplets are exported and applied to Beam 2, as these are common magnets and should share the error.
    \item A truncated Gaussian distribution is applied to the remaining quadrupoles Q\num{4} to Q\num{11} in IRs \numlist{1;2;5;8} for Beam~\num{2}.
    \item Coupling RDTs for Beam~\num{2} are calculated as done for Beam~\num{1}.
    \item The real and imaginary parts of the coupling RDTs at each BPM and for each beam are concatenated in order to obtain a single vector of values for a given sample.
\end{enumerate}

To train the model the relation is flipped such that the introduced tilt errors have to be found based on given coupling RDTs computed from the perturbed optics.
Therefore, the coupling RDTs reconstructed at each BPM are considered as model input, or features.
A vector containing the estimated \(\mathrm{DPSI}\) value attributed to each affected quadrupole is the desired output of the trained model.
A simulation data set of \num{50000} samples was divided into train and test sets (\qty{75}{\percent} and \qty{25}{\percent} respectively).
Each sample pair consists in \num{4424} inputs (real and imaginary parts of each coupling RDT for each BPM for each beam) and \num{160} outputs (\(\mathrm{DPSI}\) value at each affected IR quadrupole).

\subsection{Model Training and Evaluation}

In this study, models have been evaluated based on their \(R^2\) scores (coefficient of determination) as well as the normalized mean absolute error between the true output values and the model predictions.
In addition, in order to simulate the measurement uncertainty of the reconstructed RDTs both train and test sets were augmented by adding Gaussian noise to RDTs.

The standard deviation of the added noise was determined by a statistical analysis of several measurements from the LHC Run~\num{2}.
\Cref{figure:rdts_stdev_batch} shows the standard deviation of measured coupling RDTs across Beam~\num{1} BPMs for a batch of measurements taken on April~\num{3}, \num{2018}.

\begin{figure}[!htb]
    \centering
    \includegraphics*[width=0.99\textwidth]{Figures/ML_for_IR_Coupling/ml_rdts_batch_stdev.pdf}
    \caption{Standard deviation of the coupling RDTs at BPMs for Beam~\num{1}, from a batch of measurements taken on April~\num{3}, \num{2018}. These data points were later divided into IR BPMs and arc BPMs to determine applied noise levels.}
    \label{figure:rdts_stdev_batch}
\end{figure}

After analyzing several such batches, the following noise levels were determined:
\begin{itemize}
  \item Coupling RDTs at arc BPMs were noised with standard deviation ranging from \num{0} to \num{1E-5} absolute error.
  \item Inner BPMs located in the IRs (number \num{1} to \num{6} from the IP) were noised with standard deviation ranging from \num{0} to \num{1E-2} absolute error.
\end{itemize}

A new dataset was created for \textit{each combination} of the noise levels mentioned above.
For instance, a given set had IR BPMs noised with a standard deviation of \num{1E-3} and arc BPMs with \num{1E-6}.
Various regression models were trained and evaluated on each of these datasets.

\Cref{figure:ridge_mae_scores} shows the test performance of a Ridge Regression model~\cite{MIT:Rifkin:Regularized_Least_Squares} on noised data sets depending on the level of noise added to different BPMs.
Here the Mean Absolute Error (MAE) - the sum of absolute errors divided by the sample size - was normalized to the standard deviation of the applied tilts, \(\sigma_{\mathrm{DPSI}} =\)~\qty{1E-4}{\radian}.

\begin{figure}[!htb]
    \centering
    \includegraphics*[width=0.99\textwidth]{Figures/ML_for_IR_Coupling/ml_ridge_mae_scores.pdf}
    \caption{Normalized mean absolute error of a Ridge model on various noised data sets. The \(\sigma\) values indicated correspond to the standard deviation of the Gaussian noise distributions added to the coupling RDTs data.}
    \label{figure:ridge_mae_scores}
\end{figure}

\Cref{figure:ridge_r2_scores} shows again the test performance of the same Ridge model through its \(R^2\) scores.

\begin{figure}[!htb]
    \centering
    \includegraphics*[width=0.99\textwidth]{Figures/ML_for_IR_Coupling/ml_ridge_r2_scores.pdf}
    \caption{\(R^2\) scores of a Ridge model on various noised data sets. The \(\sigma\) values indicated correspond to the standard deviation of the Gaussian noise distributions added to the coupling RDTs data.}
    \label{figure:ridge_r2_scores}
\end{figure}

In \cref{figure:ridge_mae_scores,figure:ridge_r2_scores}, each curve represents a noise level applied to arc BPMs data while each point on these curves corresponds to a noise level applied to the inner BPMs.
In both figures the impact of noising the reconstructed coupling RDTs is noticeable, and leads to a significant decrease of the model's performance.
\todo{Talk more about results, it's good when not a lot of noise.}

\section{Conclusions and Outlooks}

