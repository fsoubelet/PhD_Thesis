\chapter{Machine Learning for Interaction Region Local Coupling} % Main chapter title
\label{chapter:ml_local_coupling} % For referencing the chapter elsewhere, use \cref{Chapter:ML_Local_Coupling}

Machine learning methods have found their application in a variety of fields ranging including technology development, scientific research, medical diagnosis and business insights.
The now widespread application of machine learning techniques demonstrates their applicability to various challenges and tasks.

In recent years machine learning has been successfully applied to various areas of accelerator physics~\cite{}, including at the LHC~\cite{PRAB:Fol:Detection_Faulty_BPMs,EPJP:Fol:Supervised_Learning_Reconstruction_Magnet_Errors,PHD:Fol:Application_ML_Beam_Optics}.
The primary purpose of the work presented hereafter is to explore the possibility of applying machine learning techniques to the subject of local IR coupling in the LHC.

\section{Relevant Concepts of Machine Learning}

Machine learning can be seen as the intersection of statistics and computer science.
T. Mitchell illustrates the key characteristics of machine learning as \textit{"machine learning techniques aim to build computer programs and algorithms that automatically improve with experience by learning from examples with respect to some class of task and performance measure, without being explicitly programmed"}~\cite{BOOK:Mitchell:Machine_Learning}.
A key characteristic distinguishing machine learning from conventional programming is the program's ability to automatically achieve performance improvements based on provided data.

This section provides a quick introduction to relevant paradigms and concepts of machine learning, primarily derived from~\cite{BOOK:Mitchell:Machine_Learning,BOOK:Hastie:Elements_Statistical_Learning} where one can find detailed discussions on the subject.
The focus hereafter is kept only on particular algorithms used in the applications presented in this chapter.

\subsection{Defining a Machine Learning Task}

In the context of machine learning, the \textit{learning} refers to the process of performance improvement.
Generally speaking, a given model can improve its performance by adjusting its parameters with respect to provided data in order to refine the approximation function that is derived from said data. 
It is important to note that a trained model will always remain an approximation of the relationship in the underlying data, but that with a good enough training this approximation should be sufficiently reliable for the specific task at hand.
The requirements on the model's performance in terms of accuracy, precision and reliability depend on a particular task and application domain.

The chosen performance criteria, the type of experience to be obtained via learning and the specific approach and algorithm are specific to the task that should be learned.
These three parameters - task, measure of performance and source of experience - are the fundamentals of a well-defined machine learning problem.
In practice, it is reflected in the selection of appropriate learning method and function approximation algorithm, definition of the loss function and its acceptable values, as well as the preparation of data.
The latter usually appears to be non-trivial and can require dedicated techniques and algorithms e.g. for feature extraction or correlation analysis.

\subsection{Supervised and Unsupervised Learning}

Two main approaches are available in machine learning depending on the problem definition and the availability and structure of learning example.

\intro{Unsupervised learning} is characterized by the use of unlabeled datasets.
Unsupervised learning algorithms solve tasks where only input data is available, and are used to analyze and cluster inputs by discovering patterns in data without the need for human intervention, hence them being \textit{unsupervised}.
They are suitable for tasks such as anomaly detection, signal denoising, dimensionality reduction, and feature extraction.
% They are used for tasks such as clustering, association and dimensionality reduction.
% Clustering groups unlabeled data based on their similarities or differences, association finds relationships between variables in a given dataset and dimensionality reduction reduces the number of data inputs to a manageable size while also preserving the data integrity.

\intro{Supervised learning} is characterized by the use of labeled datasets.
These datasets are used to train - or \textit{supervise} - the model to learn how to accurately classify data or predict outcomes.
During the training, predictions are computed from the inputs and compared to the known corresponding outputs.
By adjusting the parameters of the model - or approximation function - the quantified difference between computed predictions and correct outputs - the so-called \textit{loss function} - is minimized.
% By providing known labeled inputs and outputs, the model can measure its accuracy and adjust its parameters to improve during the learning.
Supervised learning is typically used for \intro{classification} and \intro{regression} tasks.
Classification tasks aim to accurately assign test data into specific categories, and regression tasks understand aim to understand and approximate the relationship between dependent and independent variables in order to predict the outcome of a given input.

\subsection{Generalization and Overfitting}

\subsection{Regression and Classification}

% \subsection{Neural Networks}

% \subsection{Decision Trees and Ensemble Methods}


\section{Identification of Local Coupling Sources}

In the previous chapter, the negative impact of local IR linear coupling in the LHC has been extensively discussed, as well as the necessity of its identification and mitigation.
The precise knowledge of a coupling source's location and relative strength would be a valuable asset for further correction, however it was illustrated in \cref{section:current_correction_methods_and_their_limitations} how simply looking at patterns and jumps in the coupling RDTs was not always sufficient to accurately pinpoint the  location of a source, nor can it be used in locations with little instrumentation or unfavorable conditions.
As machine learning techniques have found their application in a wide range of particle accelerator control tasks in the past~\cite{IPAC:Fol:Machine_Learning_Methods_for_Optics_Measurements_and_Corrections_at_LHC, IEEE:Edelen:Neural_Networks_Modeling_Control_Particle_Accelerators, EPAC:Bozoki:Neural_Networks_Orbit_Control_Accelerators, IPAC:Meier:Orbit_Correction_Studies_using_Neural_Networks, EPAC:Kijima:Beam_Diagnostic_System_for_Accelerator_using_Neural_Networks, PRAB:Fol:Detection_Faulty_BPMs, EPJP:Fol:Supervised_Learning_Reconstruction_Magnet_Errors}, 

\section{Conclusions and Outlooks}

