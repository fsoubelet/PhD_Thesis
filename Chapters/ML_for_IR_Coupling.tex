\chapter{Machine Learning for Interaction Region Local Coupling}
\label{chapter:ml_local_coupling}

Machine learning methods have found their application in a variety of fields including technology development, scientific research, medical diagnosis and business insights.
The now widespread application of machine learning techniques demonstrates their applicability to various challenges and tasks.

In recent years machine learning has been successfully applied to various areas of accelerator physics~\cite{PRAB:Edelen:Machine_Learning_Optimization_Particle_Accelerator_Systems,PRAB:Gao:Bayesian_Optimization_Trajectory_Alignment,PRAB:Ivanov:Physics_Based_Deep_Neural_Networks_Beam_Dynamics,PRL:Roussel:Phase_Space_Reconstruction_Neural_Networks,PRAB:Kranjcevic:Multiobjective_Optimization_Dynamic_Aperture,PRL:Jalas:Bayesian_Optimization_Laser_Plasma_Accelerator,PRL:Duris:Bayesian_Optimization_Free_Electron_Laser,IPAC:Biedron:Adding_Data_Science_and_More_Intelligence_to_Our_Accelerator_Toolbox,IEEE:Edelen:Neural_Networks_Modeling_Control_Particle_Accelerators,PRAB:Emma:Machine_Learning_Longitudinal_Phase_Space_Prediction,IBIC:Xu:Machine_Learning_Image_Processing_Technology_Application_in_Bunch_Longitudinal_Phase_Data_Information_Extraction}, including at the \gls{LHC}~\cite{PRAB:Fol:Detection_Faulty_BPMs,EPJP:Fol:Supervised_Learning_Reconstruction_Magnet_Errors,PHD:Fol:Application_ML_Beam_Optics}.
The primary purpose of the work presented hereafter is to explore the possibility of applying machine learning techniques to the subject of local \gls{IR} coupling in the LHC.

%----------------------------------------------------------------------------------------

\section{Relevant Concepts of Machine Learning}
\label{section:concepts_of_machine_learning}

Machine learning can be seen as the intersection of statistics and computer science.
The key characteristics of machine learning are described by T. Mitchell as~\cite{BOOK:Mitchell:Machine_Learning}:

\begin{quoteblock}
    Machine learning techniques aim to build computer programs and algorithms that automatically improve with experience by learning from examples with respect to some class of task and performance measure, without being explicitly programmed.
\end{quoteblock}

A key characteristic distinguishing machine learning from conventional programming is the program's ability to automatically achieve performance improvements based on provided data.
This section provides a quick introduction to relevant paradigms and concepts of machine learning, primarily derived from~\cite{BOOK:Mitchell:Machine_Learning,BOOK:Hastie:Elements_Statistical_Learning} where one can find detailed discussions on the subject.
The focus is kept only on particular concepts relevant to the work presented in this chapter.

\subsection{Defining a Machine Learning Task}

In the context of machine learning, \concept{learning} refers to the process of improving performance.
Generally speaking a model can improve its performance by adjusting its parameters with respect to provided data, in order to refine the approximation function that is derived from said data. 
It is important to note that a trained model will always remain an approximation of the relationship in the underlying data, but with a good enough training this approximation should be sufficiently reliable for the specific task at hand.
The requirements on the model's performance in terms of accuracy, precision and reliability depend on a particular task and application domain.

The chosen performance criteria, the type of experience to be gained through learning and the specific approach and algorithm are specific to the task that must be learned.
These three parameters - task, measure of performance and source of experience - are the fundamentals of a well-defined machine learning problem.
In practice, it is reflected in the selection of appropriate learning method and function approximation algorithm, definition of the loss function and its acceptable values, as well as the preparation of data.
The latter can be non-trivial and require dedicated techniques and algorithms.

\subsection{Supervised and Unsupervised Learning}

Two main approaches are available in machine learning depending on the problem definition and the availability and structure of the learning examples.

\concept{Unsupervised learning} is characterized by the use of unlabeled datasets.
Unsupervised learning algorithms solve tasks where only input data is available, and are used to analyze and cluster inputs by discovering patterns in data without the need for human intervention, hence them being \textit{unsupervised}.
They are suitable for tasks such as anomaly detection, signal denoising, dimensionality reduction, and feature extraction.
% They are used for tasks such as clustering, association and dimensionality reduction.
% Clustering groups unlabeled data based on their similarities or differences, association finds relationships between variables in a given dataset and dimensionality reduction reduces the number of data inputs to a manageable size while also preserving the data integrity.

\concept{Supervised learning} is characterized by the use of labeled datasets.
These datasets are used to train - or supervise - the model to learn how to accurately classify data or predict outcomes.
During the training, predictions are computed from the inputs and compared to the known corresponding outputs.
By adjusting the parameters of the model - or approximation function - the quantified difference between computed predictions and correct outputs, the so-called \concept{loss function}, is minimized.
% Providing known labeled inputs and outputs allows the model to measure its accuracy and adjust its parameters during learning.
Supervised learning is typically used for \concept{classification} and \concept{regression} tasks.
Classification tasks aim to accurately assign test data into specific categories, and regression tasks aim to understand and approximate the relationship between dependent and independent variables in order to predict the outcome of a given input.

\subsection{Regression Models}

In the case of regression problems, the regression function is what is being approximated by the model.
The core model principles are outlined below.

\subsubsection*{Linear Models}

Linear models compute the estimates of the output values using a linear function of the input variables~\cite{PNAS:Lai:Strong_Consistency_Least_Squares_Estimates_Regression_Models}.
For regression tasks, the estimation for the predicted output \(\hat{y}\) of a linear model is expressed as:

\begin{equation}
    \hat{y} = w_0 + \sum_{i=1}^{N} w_i x_i \text{ ,}
    \label{equation:linear_regression_prediction}
\end{equation}
where \(\vec{x}\) is the input vector containing \(N\) features, and \(\vec{w}\) is the vector containing the learned coefficients of the model, with \(w_0\) being the bias.
In a simple case (see \cref{figure:ml_underfitting_overfitting} below) \(\hat{y}\) is a scalar, but it is usually a vector of dimension \(K\).
In this case the model parameters \(w\) are then represented as an \(N \times K\) matrix.

Typically, linear regression models are trained using the \concept{least squares} method, which minimizes the residual sum of squared errors between the predicted and actual output values.
Given a set of \(N\) samples from the training dataset, and \(y\) being the true output for a given sample, the linear model's parameters are updated during training with each incoming sample, minimizing:

\begin{equation}
    RSS(\hat{w}) = \sum_{m=1}^{N} (y_m - \hat{y}_m)^2 \text{ .}
    \label{equation:least_squares_rss}
\end{equation}

After training, the resulting coefficients should be sufficiently general to generate accurate predictions for all provided data samples.

\subsubsection*{Polynomial Regression}

Some learning tasks, due to the complex properties of the data, require non-linear predictors.
For instance, this is the case when fitting a one dimensional polynomial function of degree \(n\):

\begin{equation}
    p(x) = a_0 + a_1 x + a_2 x^2 + \dots + a_n x^n \text{ ,}
    \label{equation:polynomial_function}
\end{equation}
\vspace{1pt}

\noindent
where \((a_0, \ldots, a_n)\) is a vector of coefficients of size \(n+1\).
One way to train a model to approximate such a function is to reduce the problem to linear regression~\cite{BOOK:ShalevShwartz:Understanding_Machine_Learning}.
To do so one can introduce a mapping such that \(\psi(x)=\left(1, x, x^2, \ldots, x^n\right)\), which reduces the problem to:

\begin{equation}
    p(\psi(x)) = a_0 + a_1 x + a_2 x^2 + \ldots + a_n x^n = \langle \vec{a}, \psi(x) \rangle \text{ ,}
    \label{equation:polynomial_function_linear_mapping}
\end{equation}
\vspace{1pt}

\noindent
and the optimal vector \(\vec{a}\) can be found by minimizing the residual sum of squares (RSS) as in \cref{equation:least_squares_rss}.

\subsection{Generalization, Underfitting and Overfitting}
\label{subsection:generalization_underfitting_overfitting}

In supervised learning, one tries to train a model capable of making correct predictions on new, unseen data based on the experience and tuning acquired on the training data's known outputs.
This ability to predict on new data is called \concept{generalization}, and is the most important aspect of the built model~\cite{PR:Rosenblatt:Perceptron,BOOK:Bishop:Neural_Networks_Pattern_Recognition}.
A separate and unrelated test dataset is usually used in order to evaluate the predictive power of the model.
Most of the time, training and test datasets share enough similarities - properties and relations in the underlying data - such that the model can accurately predict on the test set.

However, in the case of highly complex models high accuracy can be achieved on the training data with poor prediction performance on the test data.
When too complex, models can tend to fit very closely to individual data points in the training data and extrapolate unexisting relations that will later on perform badly on new data.
This phenomenon is called \concept{overfitting}.
Similarly, when too simple for the data at hand the model will not be able to capture the underlying relations and will not be able to perform accurate predictions even on training data.
This is referred to as \concept{underfitting}.

\Cref{figure:ml_underfitting_overfitting} illustrates these concepts by showing an underfitting model, a model that generalizes well, and an overfitting model.
Though simple, this example highlights the importance of building a model that generalizes well.

\begin{figure}[!htb]
    \centering
    \includegraphics*[width=\textwidth]{Figures/ML_for_IR_Coupling/ml_underfitting_overfitting.pdf}
    \caption{Three examples showcasing models approximating (\textcolor{mplblue}{blue}) a true function (\textcolor{mplorange}{orange}) after training on provided samples (\textcolor{mplb}{dark blue}). Of the three models one is too simple and underfits (left), one generalizes properly (middle), and one is too complex and overfits (right). This plot was reproduced from the \textit{scikit-learn} documentation~\cite{CODE:scikit-learn, Website:ScikitLearn:Overfitting_Example}.}
    \label{figure:ml_underfitting_overfitting}
\end{figure}

One of the common ways to identify underfitting or overfitting is to provide validation data to the model in addition to the training data.
The training set is then used to drive the learning procedure, and after training the error is evaluated on the validation set.
When done properly, the model's performance on both training and validation sets should be nearly equal.
If the performance on the training set is substantially higher, the model has been overfitted and further adjustments on the model complexity are then required.

\subsubsection*{Regularized Regression}

A potential approach to avoid overfitting is to apply regularization during the model training by imposing a penalty on the update of the model's parameters.
As the model parameters are updated to attempt to fit every incoming input-output pair, the penalty constrains the change of these parameters such that the model becomes more robust against variations in the data.
This technique is known as \textit{L2 regularization}, and in the context of supervised regression \concept{ridge regression}~\cite{MIT:Rifkin:Regularized_Least_Squares}.

A ridge regressor minimizes the residual sum of squares introduced above by introducing a tuning parameter \(\alpha\) which mitigates the potential linear dependencies between different input variables in the dataset.
In the presence of such dependencies, parameters might be updated inappropriately during training in response to small changes in the model or the data.
The regression task is formulated as:
\begin{equation}
    \min _w \left|\left| w \vec{x} - \vec{y} \right|\right|_2^2 + \alpha \left|\left| w \right|\right|_2^2 \text{ ,}
    \label{equation:ridge_regression_regularization}
\end{equation}
where \(w\) is the matrix containing the parameters of the model, \(\vec{x}\) is the input data vector and \(\vec{y}\) the vector of targets to be predicted by the model.
With higher values of \(\alpha\) the changes in model parameters are reduced at each update step, helping to not overreact to correlated changes in the data.

It is also possible to mitigate overfitting by decreasing the number of input features while retaining only uncorrelated features for function approximation by the model.
\concept{Lasso regression}~\cite{JRSS:Tibshirani:Regression_Shrinkage_Selection_Via_Lasso} uses an \(L1\) norm to drive some regression coefficients to \(0\), which improves the model in terms of variance and allows for automatic feature selection.

To tune additional model parameters such as the ridge penalty \(\alpha\) or the number of input features to be retained in Lasso regression one can use \concept{cross-validation}, a resampling method which consists in randomly dividing the dataset into equally sized parts.
For instance, the dataset is divided in ten subsets and the model is fitted on nine of those while the loss is computed on the remaining subset.
This procedure is repeated for each of the ten subsets to be used for loss computation, and all ten estimates of the prediction error are averaged.
As the practice is part of the training process cross-validation is only applied to the training dataset, and the test dataset is still used only to evaluate a trained model's performance on unobserved data.

\section{Identification of Local Coupling Sources}

In the previous chapter, the negative impact of local \gls{IR} linear coupling in the \gls{LHC} has been extensively discussed, as well as the necessity of its identification and mitigation.
The precise knowledge of a coupling source's location and relative strength would be a valuable asset for further correction.
However, it was illustrated in \cref{section:current_correction_methods_and_their_limitations} how simply looking at patterns and jumps in the coupling \glspl{RDT} was not always sufficient to accurately pinpoint the  location of a source, nor can it be used in locations with little instrumentation or unfavorable conditions.
As machine learning techniques have found their application in a wide range of particle accelerator control tasks in the past~\cite{IPAC:Fol:Machine_Learning_Methods_for_Optics_Measurements_and_Corrections_at_LHC, IEEE:Edelen:Neural_Networks_Modeling_Control_Particle_Accelerators, EPAC:Bozoki:Neural_Networks_Orbit_Control_Accelerators, IPAC:Meier:Orbit_Correction_Studies_using_Neural_Networks, EPAC:Kijima:Beam_Diagnostic_System_for_Accelerator_using_Neural_Networks, PRAB:Fol:Detection_Faulty_BPMs, EPJP:Fol:Supervised_Learning_Reconstruction_Magnet_Errors}, the possibility of applying machine learning techniques to the detection of local IR coupling sources in the LHC was explored.

In order to perform a prediction of betatron coupling sources' locations, one first needs to compute the \(f_{1001}\) and \(f_{1010}\) coupling RDTs which will serve as input data for the model.
The strength and variations of the coupling RDTs throughout the machine is then used to estimate the location of coupling sources and their relative strengths.

In terms of machine learning, this task can be defined as a regression problem that can be solved by training a model using measurements and corresponding solutions.
Such a regression model requires a large dataset in order to be able to generalize and produce reliable results.
As in the real machine the true location of coupling sources is unknown, no real-world data is available for the training of machine learning models.

\subsection{Dataset Generation}

In order to create a training dataset simulations were performed with the \acrshort{MAD}-X~\cite{CODE:MADX_guide} code, in which random rotations around the \(s\)-axis are introduced into individually powered IR quadrupoles, in each IR.
The tilt generates a \gls{skew} quadrupolar component at the affected element and thus turns it into a source of coupling.
It has to be noted though, that in simulations only quadrupole tilts - given by the \(\mathrm{DPSI}\) variable in MAD-X - were used to generate coupling sources, which means other potential sources such as feed-down from higher order magnets were ignored in this study.

When generating the data from simulations, the introduced tilt components form the training output and the produced coupling RDTs generated from the perturbed optics functions are the input in the training dataset.
The data was generated for both Beam 1 and 2 using the \(\beta^{\ast}=\)~\qty{30}{\centi\meter} optics.
\Cref{figure:ml_local_sources_rdts} shows the reconstructed coupling RDTs for a given simulation in which a truncated Gaussian distribution of tilts was assigned to all independently powered IR quadrupoles Q\num{1} to Q\num{11} in IRs \numlist{1;2;5;8}.

\begin{figure}[!htb]
    \centering
    \includegraphics*[width=0.99\textwidth]{Figures/ML_for_IR_Coupling/ml_local_sources_rdts_combined.pdf}
    \caption{Beam~\num{1} (top) and Beam~\num{2} (bottom) coupling RDTs \(f_{1001}\) (\textcolor{mplblue}{blue}) and \(f_{1010}\) (\textcolor{mplorange}{orange}) after the implementation of tilt errors in the independently powered IR quadrupoles Q\num{1} to Q\num{11} in IRs \numlist{1;2;5;8}, for the \qty{6.8}{\tera\electronvolt} and \(\beta^{\ast}=\)~\qty{30}{\centi\meter} optics.}
    \label{figure:ml_local_sources_rdts}
\end{figure}

The standard deviation of the applied tilt errors Gaussian distribution was aligned with expected values from the element alignment precision in the LHC, after discussions with the alignment group.
Each sample of the dataset is obtained by applying the following steps in simulations:
\begin{enumerate}
    \item A truncated Gaussian distribution of tilt errors (\(\mathrm{DPSI}\)) is applied to quadrupoles Q\num{1} to Q\num{11} in IRs \numlist{1;2;5;8} for Beam~\num{1}.
    \item Quadrupoles located outside the IRs are excluded as these sources can be identified and compensated by other means.
    \item The coupling RDTs \(f_{1001}\) and \(f_{1010}\) are calculated at each \acrshort{BPM} from \gls{Twiss_parameters} for Beam~\num{1}.
    \item The \(\mathrm{DPSI}\) values for triplets are exported and applied to Beam 2, as these are common magnets and should share the error.
    \item A truncated Gaussian distribution similar to the one of step \num{1} is applied to the remaining quadrupoles Q\num{4} to Q\num{11} in IRs \numlist{1;2;5;8} for Beam~\num{2}.
    \item Coupling RDTs for Beam~\num{2} are calculated at each BPM as done for Beam~\num{1}.
    \item The real and imaginary parts of the coupling RDTs at each BPM and for each beam are concatenated in order to obtain a single vector of values for a given sample.
\end{enumerate}

To train the model the relation is flipped, and the introduced tilt errors have to be found based on given coupling RDTs computed from the perturbed optics.
Therefore, the coupling RDTs reconstructed at each BPM are considered as model input and a vector containing the predicted \(\mathrm{DPSI}\) value attributed to each affected quadrupole is the desired output.
\Cref{figure:ml_supervised_training_schematic} shows a conceptual schematic of the dataset generation and supervised model training process for this given application.
% More specifically,each block is repeated many times: the former during dataset generation and the latter for supervised training.

\begin{figure}[!htb]
    \centering
    \includegraphics*[width=0.99\textwidth]{Figures/ML_for_IR_Coupling/supervised_training_schematic.pdf}
    \caption{Conceptual representation of the dataset generation and supervised model training. This diagram is heavily inspired from~\cite{PHD:Fol:Application_ML_Beam_Optics}.}
    \label{figure:ml_supervised_training_schematic}
\end{figure}

A dataset of \num{50000} samples (each an individual simulation following the steps above) was divided into train and test sets (\qty{75}{\percent} and \qty{25}{\percent} respectively).
Each sample pair consists in \num{4424} inputs - real and imaginary parts of each coupling RDT for each BPM for each beam - and \num{160} outputs: one \(\mathrm{DPSI}\) value at each affected IR quadrupole.

In addition, in order to reproduce the uncertainty of the RDTs reconstructed from measurements, both train and test datasets were augmented by adding Gaussian noise to the RDTs.
The standard deviation of the added noise was determined by a statistical analysis of several measurements from the LHC Run~\num{2}.
\Cref{figure:rdts_stdev_batch} shows the standard deviation of measured coupling RDTs across Beam~\num{1} BPMs for a batch of measurements taken on April~\num{3}, \num{2018}.

\begin{figure}[!htb]
    \centering
    \includegraphics*[width=0.95\textwidth]{Figures/ML_for_IR_Coupling/ml_rdts_batch_stdev.pdf}
    \caption{Standard deviation of the coupling RDTs at BPMs for Beam~\num{1}, from a batch of measurements taken on April~\num{3}, \num{2018}. These data points were later divided into IR and arc BPMs to determine applied noise levels.}
    \label{figure:rdts_stdev_batch}
\end{figure}

After analyzing several such batches, the following noise levels to be added were determined:
\begin{itemize}
  \item Coupling RDTs at arc BPMs were noised with a truncated Gaussian distribution of standard deviation ranging from \num{0} to \num{1E-5} absolute error.
  \item Inner BPMs located in the IRs (number \num{1} to \num{6} from the IP) were noised with a truncated Gaussian distribution of standard deviation ranging from \num{0} to \num{1E-2} absolute error.
\end{itemize}

A new dataset was created for each combination of the noise levels mentioned above.
For instance, a given set had IR BPMs noised with a standard deviation of \num{1E-3} and arc BPMs with \num{1E-6}.
Similarly, several such datasets were created for different standard deviation of the introduced tilt errors.

\subsection{Model Training and Evaluation}

In this study models have been evaluated based on their \(R^2\) scores (coefficient of determination) as well as the normalized mean absolute error between the true output values and the model predictions.
Several models suited for regression tasks were tested, that are listed below, and a minimal amount of hyperparameter tuning was performed.

Each model was trained and evaluated on the various datasets generated, in order to assess their performance and usability in various realistic contexts of noise in the data. 
A simple least squares linear regression~\cite{PNAS:Lai:Strong_Consistency_Least_Squares_Estimates_Regression_Models} showed very good results on clean data, however its performance dropped drastically when applied to the noised datasets, down to unusable accuracy.
A decision tree regressor~\cite{BOOK:Breiman:Classification_Regression_Trees} and a random forest regressor~\cite{ML:Breiman:Random_Forests} showed poor performance on all datasets.
A ridge regressor model (see \cref{subsection:generalization_underfitting_overfitting}) showcased good performance on both clean and relatively low-noise datasets.

\Cref{figure:ridge_performance} shows the test performance of the ridge regressor~\cite{MIT:Rifkin:Regularized_Least_Squares} on noised datasets depending on the level of noise added to different BPMs, where the impact of noise on the reconstructed coupling RDTs is apparent.
In the top plot the Mean Absolute Error (MAE) - the sum of absolute errors divided by the sample size - was normalized to the standard deviation of the applied magnet tilts, \(\sigma_{\mathrm{DPSI}} =\)~\qty{1E-4}{\radian}.
In this figure each curve represents a noise level applied to arc BPMs data while each point on these curves corresponds to a noise level applied to the inner BPMs.
While dense, this representation allows to see the performance of the model on a wide range of noised datasets, for a single given distribution of the tilt errors.

\begin{figure}[!htb]
    \centering
    \includegraphics*[width=0.95\textwidth]{Figures/ML_for_IR_Coupling/ml_ridge_performance.pdf}
    \caption{Normalized mean absolute error (top) and \(R^2\) scores (bottom) of a ridge regressor on various datasets corresponding to different noise levels added to the coupling RDTs. The \(\sigma\) values indicated correspond to the standard deviation of the Gaussian noise distributions added to the coupling RDTs data.}
    \label{figure:ridge_performance}
\end{figure}

To the left of the plot at data points corresponding to low noise levels in IR BPMs, where for two of the curves (\textcolor{mplblue}{blue} and \textcolor{mplorange}{orange}) the model's performance is excellent.
Moving up one order of magnitude in the arc BPMs noise level (\textcolor{mplgreen}{green}) leads to a significant performance drop.
Point to the right of the plot, corresponding to increasing noise in IR BPMs data, also showcase a drop in performance down to an unusable model, with \(R^2 = 0.54\) in the worst case.

\Cref{figure:ridge_histograms} shows histograms of the distributions of true attributed errors, of the ridge regressor's predictions and of deviations to the true values.
These were obtained by evaluating the model on a noised dataset with \(\sigma_{\mathrm{DPSI}} =\)~\qty{1E-4}{\radian}, \(\sigma_{\mathrm{IRs}} = 10^{-5}\) and \(\sigma_{\mathrm{arcs}} = 10^{-6}\) (corresponding to the third leftmost point on the \textcolor{mplorange}{orange} curve in \cref{figure:ridge_performance}).
One can notice the histogram of deviations much more narrowly centered on \num{0} than that of the attributed errors, which is the sign of good performance.

\begin{figure}[!htb]
    \centering
    \includegraphics*[width=0.7\textwidth]{Figures/ML_for_IR_Coupling/ridge_histograms.pdf}
    \caption{Histograms of the true applied \(\mathrm{DPSI}\) values (\textcolor{ridgepurple}{purple}), the values predicted by the Ridge model (\textcolor{ridgeblue}{blue}) and the deviations from the predictions to the true values (\textcolor{ridgesalmon}{pink}) for a noised sample in the test dataset (\(\sigma_{\mathrm{DPSI}} =\)~\qty{1E-4}{\radian} and \(\sigma_{\mathrm{Arcs, IRs}} = 10^{-5}\)).}
    \label{figure:ridge_histograms}
\end{figure}

\Cref{figure:ridge_predictions} shows the ridge model's predictions on a sample from the same noised test dataset (\(\sigma_{\mathrm{DPSI}} =\)~\qty{1E-4}{\radian}, \(\sigma_{\mathrm{IRs}} = 10^{-5}\) and \(\sigma_{\mathrm{arcs}} = 10^{-6}\)), where a good agreement between the predicted and the assigned true values can be observed.
Prediction performance significantly degrades with the addition of noise, with sometimes predictions being off by an order of magnitude or more in the worst cases.

\begin{figure}[!htb]
    \centering
    \includegraphics*[width=0.95\textwidth]{Figures/ML_for_IR_Coupling/ml_ridge_regression_sample_predictions.pdf}
    \caption{True assigned (\textcolor{mplblue}{blue}) and predicted (\textcolor{mplorange}{orange}) \(\mathrm{DPSI}\) values with a ridge model for a given test dataset sample. Here the magnet names have been switched for numbers in order to improve clarity.}
    \label{figure:ridge_predictions}
\end{figure}

Results showing the best \(R^{2}\) scores obtained by each model on both clean and noised datasets are presented in \cref{table:ml_models_comparison}, where the ridge regressor clearly outperforms its counterparts.

\begin{table}[!hbt]
    \centering
    \begin{tblr}{colspec={ccc}}
        \hline
        \textbf{Model}             &   Clean Data     &   Noised Data     \\
        \hline
        Ridge Regressor            &   \num{0.9911}   &   \num{0.8934}    \\
        Linear Regression          &   \num{0.9913}   &   \num{0.5638}    \\
        Decision Tree Regressor    &   \num{0.1385}   &   \num{-0.0018}   \\
        Random Forest Regressor    &   \num{0.0175}   &   \num{-0.0009}   \\
        \hline
    \end{tblr}
    \caption{Comparison of the \(R^2\) score averaged over \num{1000} samples taken from the test dataset for different models. For the results in this table, the standard deviations of the applied noise were \(\sigma = 10^{-4}\) for IR BPMs and  \(\sigma = 10^{-6}\) for arc BPMs. The distribution of tilt errors had a standard deviation of \(\sigma =\)~\qty{1E-4}{\radian}.}
    \label{table:ml_models_comparison}
\end{table}

From the results on clean data or low noise levels in \cref{figure:ridge_performance,table:ml_models_comparison}, one can view this study as a good proof of principle for the application of machine learning to coupling sources detection.
However, with realistic noise levels even the best performing model is not yet good enough to be used in operation or incorporated in correction techniques.
Nonetheless, while predictions might not yield accurate numbers they still provide a good indication of suspected locations of coupling sources, which may be valuable for the alignment group.

There is also potential for performance improvements.
Assigning more computing resources to the determination of model parameters through hyperparameter tuning is a first step towards better performance, but will not circumvent certain models' shortcomings.
More complex models such as Convolutional Neural Networks (CNNs) have in the past been successfully used with impressive success on regression tasks - such as in \acrshort{HEP}~\cite{JOI:Aurisano:Convolutional_Neural_Network_Neutrino_Event_Classifier} and recently in optics measurements studies~\cite{IPAC:Fol:Optics_Corrections_Using_Machine_Learning_in_the_LHC} - and would be a promising tool that could yield better prediction accuracy especially on noised datasets.
Another avenue of improvements would be to create a pipeline where first a denoising step is applied on the coupling RDTs data using another model trained for the task - this has been successfully done with auto-encoder neural networks~\cite{IPAC:Fol:Denoising_Optics_Measurements_Autoencoder_Neural_Networks} - before feeding the results to the above prediction models.
With good enough noise cleaning, the prediction models should reach the level of performance displayed in this study on clean or low noise datasets.

Currently, the main limitation of these trained models comes from the fact that only skew quadrupole components were considered in the study for the introduction of coupling.
In the real machine, other effects such as field errors in higher order magnets or orbit offsets in sextupoles would also lead to a coupling contribution, which the models would try to and blame on a quadrupole.
However, the effect of higher orders to coupling is small and assuming most contributions are in the form of tilted quadrupoles is reasonable.
Circumventing the limitation would be feasible but need much more complex and inclusive simulations.

\section{Summary}
\label{section:ml_conclusions_outlooks}

Following the successful application of machine learning methods to various tasks in particle accelerators in recent years, we explored the possibility of training machine learning models to predict linear coupling sources in the \gls{LHC} \glspl{IR} in the form of quadrupole tilts.
Several datasets were generated for training, corresponding to realistic tilt error distributions and noise levels aiming to reproduce measurements uncertainty.

We have shown that specific machine learning models among those tested are capable of predicting the IR quadrupole tilts by assigning a representative value to specific magnets with good accuracy on datasets generated from simulations.
A ridge regressor shows the best performance among the tested models, including on datasets with reasonably small amounts of noise.

While usability in operation would require better accuracy on datasets with higher noise, this is a successful proof of concept for the application of machine learning techniques to the subject of local coupling corrections in particle accelerators.
Potential improvements such as using previously successful but more complex models and workflows have been identified which could allow to improve the performance of models discussed in this study.
On the other hand, upgrading to better beam instrumentation that would yield more precise reconstructed coupling \acrshortpl{RDT} would also allow using the currently developed models with confidence.

\glsresetall                                     % reset glossary entries counts for the next chapter
